---
title: "30 Day Chart Challenge"
author: "Simon"
date: "06/04/2021"
output: html_document
---

```{r setup, include=FALSE}
myPaths <- .libPaths()
myPaths <- c(myPaths, "D:/Libraries")
myPaths <- c(myPaths, "D:\\Libraries\\win-library\\4.0")
.libPaths(myPaths)  # add new path
library(readODS)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(rvest) 
library(magrittr)
library(waffle)
library(readxl)
library(streamgraph)
library(ggstream)
library(data.table)
library(ggforce)
library(cowplot)
library(gridExtra)
library(ggplot2)

```

## 30 Day Chart Challenge 

This R Markdown document will contain a visualisation for each day within the #30daychartchallenge. 
https://twitter.com/30DayChartChall

![All 30 days themes](D:\\30daychartchallenge\\themes.jpg) 

# Day 1- Part to Whole

Using Openly available data from the National Travel Survey on Car Availability by Household

![NTS Data](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/905929/nts0203.ods)
```{r Day 1} 
# Read in the Data
NTS<- read_ods("D:\\30daychartchallenge\\30daychart\\data\\day1data_2.ods", sheet=1, skip=10,col_names=TRUE)

## Data Cleaning ##
# Remove NA Rows 
NTS <- NTS[!is.na(NTS$Purpose) & !is.na(NTS$`No car / van`) & !is.na(NTS$`One car / van`) & !is.na(NTS$`Two or more cars / vans`),]
# Remove extra columns 
NTS <- NTS[,1:4]
# Remove unwanted rows 
NTS <- NTS[1:18,]

# Set data as numeric 
NTS[,2:4] <- sapply(NTS[,2:4],as.numeric)

# Convert data to wide format to enable ggplot 
NTS_wide <- NTS %>% gather(`Car Availability`, Percentage, `No car / van`:`Two or more cars / vans`, -Purpose)
# Set column names 
colnames(NTS_wide) <- c("Year", "Car Availability", "Percentage")
## Data Visualisation ## 
ggplot(data=NTS_wide, aes(x=Year, y=Percentage, group=`Car Availability`, fill=`Car Availability`)) + 
  geom_bar(position="stack", stat="identity") + theme(axis.text.x=element_text(angle=90)) + labs(title="National Travel Survey- Car Availability by Household") + ylab("% Availability") + xlab("Year of Response") + geom_text(aes(label=round(Percentage,1)), size=2.5, color="dark grey", position = position_stack(vjust=0.5)) + scale_fill_viridis_d()

```


# Day 2- Pictogram

Using Openly available data from Wikipedia on Finalists in the UEFA Champions League

![Champions League Winners](https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals)
```{r Day 2, fig.height=40, fig.width=25}
# Read in the Data
Champions_table <- read_html("https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals") %>% 
    # list all tables on the page
    html_nodes(css = "table") %>%
    # select the one containing needed key words
    extract2(., str_which(string = . , pattern = "Seasons won")) %>% 
    # convert to a table
    html_table(fill = T)

# Keep Club Name, Title(s) and Runners-Up
Champions_table <- Champions_table[,1:3]

## Data Visualisation ## 
Champions_table$ID <- (1:nrow(Champions_table))

# Create list to store plots to 
p <- list()
# Loop through top 10 winners
for(i in 1:10){
  p[[i]] <- waffle(c("Champions"= Champions_table$`Title(s)`[i],"Runners-Up"= Champions_table$`Runners-up`[i]), rows=2, glyph_size = 4, title=paste0(Champions_table$Club[i]), colors = c("#FFD700", "#C0C0C0", NA))
}

do.call(grid.arrange,p)

```


# Day 3- Historical

Using Openly available data from Human Development Reports by the UN on Gender Development Index

![HDI Index](http://hdr.undp.org/en/composite/trends)
```{r Day 3} 
# Read in the Data
url <- 'http://hdr.undp.org/sites/default/files/2020_statistical_annex_table_2.xlsx'
GPI <- rio::import(file = url,which = 1, skip=5) %>% 
  glimpse()

## Data Cleaning ## 
# Retain columns of use
GPI <- GPI[,c(1,2,3,5,7,9,11,13,15,17)]
# Rename columns 
colnames(GPI) <- c("HDI Rank", "Country", "1990","2000", "2010", "2014", "2015", "2017", "2018", "2019")
# Remove anything without a HDI Rank 
GPI <- GPI[!is.na(GPI$`HDI Rank`),]

# Convert data to wide format to enable ggplot 
GPI_wide <- GPI %>% gather(`Year`, Percentage, `1990`:`2019`, -Country)

GPI_wide$Percentage <- as.numeric(GPI_wide$Percentage)
## Data Visualisation ## 

GPI_wide2 <- GPI_wide[GPI_wide$`HDI Rank`>=180,]
GPI_wide2$Year<-as.numeric(GPI_wide2$Year)

GPI_wide24 <- GPI_wide2 %>% 
  group_by(Year, Country) %>% 
  summarise(n=sum(Percentage)) %>% 
  mutate(percentage_2 = n/sum(n))

ggplot(data=GPI_wide24, aes(x=Year,y=percentage_2, fill=Country)) + xlab("Year") + ylab("HDI") + labs(title="Change in HDI relative to worst 9 countries since 2010 \n(proportion of total HDI Index)") + scale_fill_viridis_d() + geom_area(alpha=0.6 , size=1, colour="black")
```

# Day 4- Magical

Using Openly available data from Google Search Trends Worldwide in past 12 months for Witches and Wizards

![Google Search Trends](https://trends.google.com/trends/explore?q=wizard)
```{r Day 4} 
# Read in the Data
data_1<- fread("D:\\30daychartchallenge\\30daychart\\data\\day4data-1.csv")
data_2<- fread("D:\\30daychartchallenge\\30daychart\\data\\day4data-2.csv")

# Join datasets together
magic_data <- merge(data_1, data_2, by="Week")
colnames(magic_data) <- c("Week", "Witches", "Wizards")
# Convert data to wide format to enable ggplot 
magic_wide <- magic_data %>% gather(`Trend`, Popularity, `Witches`:`Wizards`, -Week)

# Normalise so equal search popularity means 0 
magic_wide$norm <- magic_data$Witches-magic_data$Wizards
# Remove duplicates data
magic_wide <- magic_wide[!duplicated(magic_wide$norm), ]

## Data Visualisation ## 

ggplot(data=magic_wide, aes(x=Week, y=norm)) + geom_line(size=0.9) + theme_dark() + theme(plot.background = element_rect("black")) + theme(panel.background = element_rect("black")) + theme(axis.text = element_text(color="white")) + geom_ribbon(data=,aes(x=Week,ymax=norm),ymin=0,alpha=0.3) +
theme(legend.position="none") + theme(panel.grid = element_blank()) + labs(title="Difference in Google Search Popularity between Witches and Wizards. \n Positive value means Witches were more popular that week") + ylab("Google Search Trend Popularity") + xlab("Week") + theme(title=element_text(color="white")) + geom_link2(lwd=1.5,
             aes(color = after_stat(y < 0)))+
  scale_color_manual(
    values = c("yellow", "blue")) 

```
# Day 5- Slope

Using Openly available data from the National Travel Survey for 17-20 Year Olds on reasons why they do not complete driving license examination. 

![NTS Data](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/905929/nts0203.ods)
```{r Day 5, fig.height=10, fig.width=10}
# Read in the Data
NTS<- read_ods("C:\\Users\\simon\\Downloads\\nts0203.ods", sheet=2, skip=8,col_names=TRUE)

## Data Cleaning ##
# Find which row begins and ends the data we need (using 17-20 data)
row_begin <- which(NTS$Reasons=="Reasons for not learning to drive by age: England, 17-20")
row_end <- which(NTS$Reasons=="Driving without a licence")

# Find which 'Unweighted sample size:' is closest to row_begin
row_end <- row_end - row_begin
row_end <- min(row_end[row_end > 0])
row_end <- row_end + row_begin
# Subset to 17-20 data
NTS <- NTS[row_begin: row_end,]
# Remove top 5 rows of info data 
NTS <- NTS[5:20,]

# Set data as numeric 
NTS[,2:12] <- sapply(NTS[,2:12],as.numeric)

# Convert data to wide format to enable ggplot 
NTS_wide <- NTS %>% gather(Time, Percentage, `2009`:`2019`, -Reasons)

## Data Analysis ## 
# Visualise data to understand it and add linear regression line
ggplot(data=NTS_wide, aes(x=Time, y=Percentage, group=Reasons, color=Reasons)) + 
  geom_smooth(method="lm", linetype="dashed", se=T, color="black") + 
  geom_line(stat="identity", size=0.9) + theme_classic()  + facet_wrap(~Reasons, labeller=label_wrap_gen()) + theme(legend.position="none") + theme(axis.text.x=element_text(angle=90)) + labs(title="National Travel Survey- Reasons Not to Complete Driving Licence") + ylab("% of 17-20 Year Olds") + xlab("Year of Response") + scale_color_viridis_d()

```
# Day 6- Experimental

Using Openly available data from Wikipedia on 2017 Per Capita CO2 Emissions

![Emissions Data](https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions#Per_capita_CO2_emissions)

```{r Day6, fig.height=15, fig.width=15}

# Read in the Data
Emissions_table <- read_html("https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions#Per_capita_CO2_emissions") %>% 
    # list all tables on the page
    html_nodes(css = "table") %>%
    # select the one containing needed key words
    extract2(., str_which(string = . , pattern = "Afghanistan")) %>% 
    # convert to a table
    html_table(fill = T)

# Remove top 4 rows 
Emissions_table <- Emissions_table[5:nrow(Emissions_table),]

# Keep column 1 and 8 (per capita)
Emissions_table <- Emissions_table[, c(1,8)]
#Set column names
colnames(Emissions_table) <-c("Country", "val")
# Set to numeric
Emissions_table$val<-as.numeric(Emissions_table$val)

# Sort by value of Per Capita Emissions
Emissions_table <- Emissions_table[order(Emissions_table$val),]

# Add Continent
continent <- fread("D:\\30daychartchallenge\\30daychart\\data\\day6continent.csv")
continent <- continent[,c(1,6)]

# Data Cleaning on Country Name to match them 

# Join together 
Emissions_table <- merge(Emissions_table, continent, by.x="Country", by.y="country", all.x=FALSE)
Emissions_table$continent <- as.factor(Emissions_table$continent)


# Set a number of 'empty bar' to add at the end of each group
empty_bar <- 4
to_add <- data.frame( matrix(NA, empty_bar*nlevels(Emissions_table$continent), ncol(Emissions_table)) )
colnames(to_add) <- colnames(Emissions_table)
to_add$continent <- rep(levels(Emissions_table$continent), each=empty_bar)
Emissions_table <- rbind(Emissions_table, to_add)
Emissions_table <- Emissions_table %>% arrange(continent, val)
Emissions_table$id <- seq(1, nrow(Emissions_table))

# ----- This section prepare a dataframe for labels ---- #
# Get the name and the y position of each label
label_data <- Emissions_table
 
# calculate the ANGLE of the labels
number_of_bar <- nrow(label_data)
angle <-  90 - 360 * (label_data$id-0.5) /number_of_bar     # I subtract 0.5 because the letter must have the angle of the center of the bars. Not extreme right(1) or extreme left (0)
 
# calculate the alignment of labels: right or left
# If I am on the left part of the plot, my labels have currently an angle < -90
label_data$hjust<-ifelse( angle < -90, 1, 0)
 
# flip angle BY to make them readable
label_data$angle<-ifelse(angle < -90, angle+180, angle)

# Create Label 
label_data$label <- paste0(label_data$Country, " ", label_data$val)
# ----- ------------------------------------------- ---- #

# prepare a data frame for base lines
base_data <- Emissions_table %>% 
  group_by(continent) %>% 
  summarize(start=min(id), end=max(id) - empty_bar) %>% 
  rowwise() %>% 
  mutate(title=mean(c(start, end)))


# Make the plot
p <- ggplot(Emissions_table, aes(x=as.factor(id), y=val, fill=continent)) +       # Note that id is a factor. If x is numeric, there is some space between the first bar
  geom_bar(stat="identity", alpha=0.5) +
  ylim(-100,120) +
  theme_minimal() + 
  labs(title="2017 Per Capita CO2 Emissions by Country") +
  theme(
    legend.position = "none",
    text=element_text(color="white"),
    axis.text = element_blank(),
    #axis.title = element_blank(),
    panel.grid = element_blank(),
    plot.background = element_rect(fill="black"),
    plot.title=element_text(size=12, hjust=0.2, face='bold', color="white"),
    plot.margin = unit(rep(-1,4), "cm") 
  ) +
  coord_polar()  + 
  # Add the labels, using the label_data dataframe that we have created before
  geom_text(data=label_data, aes(x=id, y=val+10, label=label, hjust=hjust), color="white", fontface="italic",alpha=0.6, size=2.5, angle= label_data$angle, inherit.aes = FALSE, position=position_jitter(width=0.1,height=0.1))  + # Add base line information
geom_segment(data=base_data, aes(x = start, y = -5, xend = end, yend = -5), colour = "white", alpha=0.8, size=0.6 , inherit.aes = FALSE )  +
geom_text(data=base_data, aes(x = title, y = -18, label=continent), hjust=c(1,1,0,0,0), colour = "white", alpha=0.8, size=4, fontface="bold", inherit.aes = FALSE)
 
 
p
```























